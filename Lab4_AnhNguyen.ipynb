{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/anhnguyen/Documents/GitHub/2025AAE722_AnhNguyen/2025 AAE722 Anh Nguyen Submission'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  default student      balance        income\n",
      "0      No      No   729.526495  44361.625074\n",
      "1      No     Yes   817.180407  12106.134700\n",
      "2      No      No  1073.549164  31767.138947\n",
      "3      No      No   529.250605  35704.493935\n",
      "4      No      No   785.655883  38463.495879\n",
      "Dataset Dimensions: (10000, 4)\n",
      "\n",
      "Column Names and Data Types:\n",
      "default    category\n",
      "student    category\n",
      "balance     float64\n",
      "income      float64\n",
      "dtype: object\n",
      "\n",
      "Distribution of 'default' variable:\n",
      "default\n",
      "No     9667\n",
      "Yes     333\n",
      "Name: count, dtype: int64\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078577\n",
      "         Iterations 10\n",
      "\n",
      "Coefficient for balance: 0.00573650526579908\n",
      "\n",
      "Interpretation: For a one-unit increase in balance, the log-odds of defaulting increase by 0.00573650526579908\n"
     ]
    }
   ],
   "source": [
    "from ISLP import load_data\n",
    "import pandas as pd\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load the Default dataset\n",
    "default_data = load_data('Default')\n",
    "\n",
    "#print the first few rows of the dataset\n",
    "print(default_data.head())\n",
    "\n",
    "# Examine the structure of the dataset\n",
    "print(\"Dataset Dimensions:\", default_data.shape)\n",
    "print(\"\\nColumn Names and Data Types:\")\n",
    "print(default_data.dtypes)\n",
    "print(\"\\nDistribution of 'default' variable:\")\n",
    "print(default_data['default'].value_counts())\n",
    "\n",
    "# Prepare the data for logistic regression\n",
    "default_data['student'] = default_data['student'].map({'Yes': 1, 'No': 0})\n",
    "default_data['default'] = default_data['default'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "X = default_data[['income', 'balance', 'student']]\n",
    "X = sm.add_constant(X)  # Add constant for the intercept\n",
    "y = default_data['default']\n",
    "\n",
    "# Fit the logistic regression model\n",
    "logit_model = sm.Logit(y, X)\n",
    "result = logit_model.fit()\n",
    "\n",
    "# Report the coefficient for balance\n",
    "balance_coef = result.params['balance']\n",
    "print(\"\\nCoefficient for balance:\", balance_coef)\n",
    "print(\"\\nInterpretation: For a one-unit increase in balance, the log-odds of defaulting increase by\", balance_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Class Means:\n",
      "[[33681.79366744   802.15837363]\n",
      " [31570.35768985  1768.16582059]]\n",
      "\n",
      "LDA Prior Probabilities:\n",
      "[0.96585714 0.03414286]\n",
      "\n",
      "LDA Confusion Matrix:\n",
      "[[2900    6]\n",
      " [  75   19]]\n",
      "\n",
      "LDA Test Accuracy: 0.973\n",
      "\n",
      "QDA Confusion Matrix:\n",
      "[[2898    8]\n",
      " [  70   24]]\n",
      "\n",
      "QDA Test Accuracy: 0.974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anhnguyen/Library/Python/3.9/lib/python/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/anhnguyen/Library/Python/3.9/lib/python/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/anhnguyen/Library/Python/3.9/lib/python/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Split the dataset into training (70%) and testing (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    default_data[['income', 'balance']], default_data['default'], test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Fit the Linear Discriminant Analysis (LDA) model\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "# Report class means and prior probabilities for LDA\n",
    "print(\"LDA Class Means:\")\n",
    "print(lda.means_)\n",
    "print(\"\\nLDA Prior Probabilities:\")\n",
    "print(lda.priors_)\n",
    "\n",
    "# Generate predictions on the test set for LDA\n",
    "y_pred_lda = lda.predict(X_test)\n",
    "\n",
    "# Create confusion matrix and calculate test accuracy for LDA\n",
    "conf_matrix_lda = confusion_matrix(y_test, y_pred_lda)\n",
    "accuracy_lda = accuracy_score(y_test, y_pred_lda)\n",
    "print(\"\\nLDA Confusion Matrix:\")\n",
    "print(conf_matrix_lda)\n",
    "print(\"\\nLDA Test Accuracy:\", accuracy_lda)\n",
    "\n",
    "# Fit the Quadratic Discriminant Analysis (QDA) model\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions on the test set for QDA\n",
    "y_pred_qda = qda.predict(X_test)\n",
    "\n",
    "# Create confusion matrix and calculate test accuracy for QDA\n",
    "conf_matrix_qda = confusion_matrix(y_test, y_pred_qda)\n",
    "accuracy_qda = accuracy_score(y_test, y_pred_qda)\n",
    "print(\"\\nQDA Confusion Matrix:\")\n",
    "print(conf_matrix_qda)\n",
    "print(\"\\nQDA Test Accuracy:\", accuracy_qda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Confusion Matrix:\n",
      "[[2893   13]\n",
      " [  75   19]]\n",
      "\n",
      "Naive Bayes Test Accuracy: 0.9706666666666667\n",
      "\n",
      "LDA Test Accuracy: 0.973\n",
      "QDA Test Accuracy: 0.974\n",
      "Naive Bayes has the lowest test accuracy.\n",
      "\n",
      "Predicted class probabilities (order = [np.int64(0), np.int64(1)]):\n",
      "[[0.48919461 0.51080539]]\n",
      "\n",
      "Predicted probability of default (class=1) for income=40000, balance=2000: 0.5108053902363209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anhnguyen/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GaussianNB was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Fit Gaussian Naive Bayes using the same training split (income and balance)\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred_nb = nb.predict(X_test)\n",
    "\n",
    "# Confusion matrix and accuracy for Naive Bayes\n",
    "conf_matrix_nb = confusion_matrix(y_test, y_pred_nb)\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "\n",
    "print(\"Naive Bayes Confusion Matrix:\")\n",
    "print(conf_matrix_nb)\n",
    "print(\"\\nNaive Bayes Test Accuracy:\", accuracy_nb)\n",
    "\n",
    "\n",
    "# Compare with LDA and QDA\n",
    "print(\"\\nLDA Test Accuracy:\", accuracy_lda)\n",
    "print(\"QDA Test Accuracy:\", accuracy_qda)\n",
    "if accuracy_nb > accuracy_lda and accuracy_nb > accuracy_qda:\n",
    "    print(\"Naive Bayes has the highest test accuracy.\")\n",
    "elif accuracy_nb < accuracy_lda and accuracy_nb < accuracy_qda:\n",
    "    print(\"Naive Bayes has the lowest test accuracy.\")\n",
    "else:\n",
    "    print(\"Naive Bayes accuracy is between LDA and QDA (or ties).\")\n",
    "\n",
    "# Predicted probability of default for a customer with income=40000 and balance=2000\n",
    "customer = [[40000, 2000]]  # columns: [income, balance]\n",
    "proba = nb.predict_proba(customer)\n",
    "# find probability for class '1' (default)\n",
    "if 1 in list(nb.classes_):\n",
    "    idx_default = list(nb.classes_).index(1)\n",
    "    proba_default = proba[0, idx_default]\n",
    "else:\n",
    "    proba_default = None\n",
    "\n",
    "print(\"\\nPredicted class probabilities (order = {}):\".format(list(nb.classes_)))\n",
    "print(proba)\n",
    "print(\"\\nPredicted probability of default (class=1) for income=40000, balance=2000:\", proba_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    K  Test Accuracy\n",
      "0   1       0.953667\n",
      "1   3       0.966333\n",
      "2   5       0.969667\n",
      "3  10       0.972000\n",
      "\n",
      "Best K: 10 with Test Accuracy: 0.972\n",
      "\n",
      "Explanation: Very small values of K, such as K=1, may not be optimal because they are highly sensitive to noise in the training data. This can lead to overfitting, where the model performs well on the training set but poorly on unseen data.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define K values to test\n",
    "k_values = [1, 3, 5, 10]\n",
    "accuracy_knn = {}\n",
    "\n",
    "# Fit and evaluate KNN models for each K value\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    accuracy_knn[k] = knn.score(X_test_scaled, y_test)\n",
    "\n",
    "# Create a summary table\n",
    "accuracy_table = pd.DataFrame(list(accuracy_knn.items()), columns=['K', 'Test Accuracy'])\n",
    "print(accuracy_table)\n",
    "\n",
    "# Identify the best K\n",
    "best_k = max(accuracy_knn, key=accuracy_knn.get)\n",
    "print(f\"\\nBest K: {best_k} with Test Accuracy: {accuracy_knn[best_k]}\")\n",
    "\n",
    "# Explanation for small K values\n",
    "print(\"\\nExplanation: Very small values of K, such as K=1, may not be optimal because they are highly sensitive to noise in the training data. This can lead to overfitting, where the model performs well on the training set but poorly on unseen data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078256\n",
      "         Iterations 10\n",
      "                Method  Test Accuracy\n",
      "0  Logistic Regression       0.973333\n",
      "1                  LDA       0.973000\n",
      "2                  QDA       0.974000\n",
      "3          Naive Bayes       0.970667\n",
      "4           KNN (K=10)       0.972000\n"
     ]
    }
   ],
   "source": [
    "# Refit Logistic Regression on training data\n",
    "logit_model_refit = sm.Logit(y_train, sm.add_constant(X_train))\n",
    "result_refit = logit_model_refit.fit()\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_logit = result_refit.predict(sm.add_constant(X_test))\n",
    "y_pred_logit_class = (y_pred_logit >= 0.5).astype(int)\n",
    "\n",
    "# Calculate test accuracy for Logistic Regression\n",
    "accuracy_logit = accuracy_score(y_test, y_pred_logit_class)\n",
    "\n",
    "# Create a summary table\n",
    "methods = ['Logistic Regression', 'LDA', 'QDA', 'Naive Bayes', f'KNN (K={best_k})']\n",
    "accuracies = [accuracy_logit, accuracy_lda, accuracy_qda, accuracy_nb, accuracy_knn[best_k]]\n",
    "\n",
    "summary_table = pd.DataFrame({'Method': methods, 'Test Accuracy': accuracies})\n",
    "print(summary_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA: FNR = 0.7979\n",
      "QDA: FNR = 0.7447\n",
      "Naive Bayes: FNR = 0.7979\n",
      "\n",
      "Lowest false negative rate: QDA (FNR = 0.7447)\n"
     ]
    }
   ],
   "source": [
    "# Compute false negative rates from the confusion matrices in the notebook\n",
    "cms = {\n",
    "    'LDA': conf_matrix_lda,\n",
    "    'QDA': conf_matrix_qda,\n",
    "    'Naive Bayes': conf_matrix_nb\n",
    "}\n",
    "\n",
    "fnr = {}\n",
    "for name, cm in cms.items():\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    fnr[name] = fn / (fn + tp) if (fn + tp) > 0 else float('nan')\n",
    "\n",
    "for name, rate in fnr.items():\n",
    "    print(f\"{name}: FNR = {rate:.4f}\")\n",
    "\n",
    "best = min(fnr, key=fnr.get)\n",
    "print(f\"\\nLowest false negative rate: {best} (FNR = {fnr[best]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recommended method is QDA because it has the lowest false negative rate (FNR = 0.7447).\n",
      "This minimizes the cost of missing a default, which is 10 times higher than a false alarm.\n"
     ]
    }
   ],
   "source": [
    "# Identify the method with the lowest FNR\n",
    "best_method = min(fnr, key=fnr.get)\n",
    "lowest_fnr = fnr[best_method]\n",
    "\n",
    "print(f\"The recommended method is {best_method} because it has the lowest false negative rate (FNR = {lowest_fnr:.4f}).\")\n",
    "print(\"This minimizes the cost of missing a default, which is 10 times higher than a false alarm.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA (threshold=0.5): FP = 8 FN = 70 FPR = 0.0028 FNR = 0.7447\n",
      "QDA (threshold=0.3): FP = 48 FN = 53 FPR = 0.0165 FNR = 0.5638\n",
      "\n",
      "Change when lowering threshold 0.5 -> 0.3:\n",
      "ΔFP = 40 ΔFN = -17 ΔFPR = +0.0138 ΔFNR = -0.1809\n"
     ]
    }
   ],
   "source": [
    "# Evaluate QDA with a lowered probability threshold (0.3) and compare to the default (0.5)\n",
    "proba_qda = qda.predict_proba(X_test)\n",
    "idx_pos = list(qda.classes_).index(1)\n",
    "probs_pos = proba_qda[:, idx_pos]\n",
    "\n",
    "# Predictions at threshold 0.3\n",
    "y_pred_qda_03 = (probs_pos >= 0.3).astype(int)\n",
    "\n",
    "# Confusion matrix and rates for threshold = 0.3\n",
    "cm_03 = confusion_matrix(y_test, y_pred_qda_03)\n",
    "tn_03, fp_03, fn_03, tp_03 = cm_03.ravel()\n",
    "fpr_03 = fp_03 / (fp_03 + tn_03) if (fp_03 + tn_03) > 0 else float('nan')\n",
    "fnr_03 = fn_03 / (fn_03 + tp_03) if (fn_03 + tp_03) > 0 else float('nan')\n",
    "\n",
    "# Baseline (threshold = 0.5) using existing conf_matrix_qda\n",
    "tn_05, fp_05, fn_05, tp_05 = conf_matrix_qda.ravel()\n",
    "fpr_05 = fp_05 / (fp_05 + tn_05) if (fp_05 + tn_05) > 0 else float('nan')\n",
    "fnr_05 = fn_05 / (fn_05 + tp_05) if (fn_05 + tp_05) > 0 else float('nan')\n",
    "\n",
    "print(\"QDA (threshold=0.5): FP =\", int(fp_05), \"FN =\", int(fn_05), f\"FPR = {fpr_05:.4f}\", f\"FNR = {fnr_05:.4f}\")\n",
    "print(\"QDA (threshold=0.3): FP =\", int(fp_03), \"FN =\", int(fn_03), f\"FPR = {fpr_03:.4f}\", f\"FNR = {fnr_03:.4f}\")\n",
    "\n",
    "print(\"\\nChange when lowering threshold 0.5 -> 0.3:\")\n",
    "print(\"ΔFP =\", int(fp_03 - fp_05), \"ΔFN =\", int(fn_03 - fn_05),\n",
    "    f\"ΔFPR = {fpr_03 - fpr_05:+.4f}\", f\"ΔFNR = {fnr_03 - fnr_05:+.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
